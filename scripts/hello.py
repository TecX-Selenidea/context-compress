# scripts/hello.py
# Purpose: quick test that your environment and Hugging Face tools work

import time
from datasets import load_dataset
from transformers import AutoTokenizer
import evaluate

print("\nðŸš€ Step 1/4: Loading a small dataset (CNN/DailyMail)...")
ds = load_dataset("cnn_dailymail", "3.0.0", split="validation[:50]")
print(f"âœ… Loaded {len(ds)} examples")

print("\nðŸ§  Step 2/4: Loading tokenizer...")
tok = AutoTokenizer.from_pretrained("distilbert-base-uncased")
sample = ds[0]["article"][:2000]
tokens = tok(sample, truncation=True, max_length=512)["input_ids"]
print(f"âœ… Tokenized length: {len(tokens)} tokens")

print("\nâœ‚ï¸ Step 3/4: Creating a simple truncation baseline...")
def truncate(text: str, n_tokens: int = 200) -> str:
    ids = tok(text, add_special_tokens=False)["input_ids"][:n_tokens]
    return tok.decode(ids)

preds = [truncate(x["article"], 200) for x in ds]
refs = [x["highlights"] for x in ds]

print("\nðŸ“Š Step 4/4: Computing ROUGE scores (quality metric)...")
rouge = evaluate.load("rouge")
score = rouge.compute(predictions=preds, references=refs)
print("âœ… ROUGE results:", score)
print("\nðŸŽ‰ Environment works! Youâ€™re ready for Day 2.")
